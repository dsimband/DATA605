---
title: "DATA605_w9_Distributions and Densities / Expected Value and Variance"
author: "David Simbandumwe"
output:
  prettydoc::html_pretty:
    theme: architect
    highlight: github
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
rm(list=ls())

```


```{r, include=FALSE}

library(matlib) 
library(pracma)
library(stats)
library(scales)

```




# Question 1 
Chapter 9 - 11 A tourist in Las Vegas was attracted by a certain gambling game in which the customer stakes 1 dollar on each play; a win then pays the customer 2 dollars plus the return of her stake, although a loss costs her only her stake. Las Vegas insiders, and alert students of probability theory, know that the probability of winning at this game is 1/4. When driven from the tables by hunger, the tourist had played this game 240 times. 

Facts
- winning probability 1/4
- stakes 1 each hand
- win 2 dollars + stake = return 3
- pays 240 times



a) Assuming that no near miracles happened, about how much poorer was the tourist upon leaving the casino? 
```{r}
p <- 1/4
s <- 1
w <- 3
n <- 240

e <- p*3 + (1-p) * (-s)

exp_ret <- e * n

print(paste0('11.a) after ' ,n,' games the expected return = $', exp_ret ))

```




b) What is the probability that she lost no money?

```{r}

sd <- sqrt(n*p*(1-p))
mu <- 0
ret <- 0
prob <- pnorm(ret, mu, sd)


print(paste0('11.b) The probability that the tourist looses no money = ', prob ))


```







# Question 2
Calculate the expected value and variance of the binomial distribution using the moment generating function.

|		The moment function for a binomial distribution can be described as follows:
$$
M_{x}(t) = E(e^{tx}) \\
M_{x}(t) = \sum_{x} e^{tx} p(x)  \\
M_{x}(t) = \sum_{x} e^{tx} \binom{n}{k} p^k (1-p)^{n-k}  \\
M_{x}(t) = \sum_{x} \binom{n}{k} (pe^{t})^k (1-p)^{n-k} = (pe^t + (1-p))^n \\
$$

|		The expected value is the first moment with t set 0
$$
M'(t) = n[pe^t + (1-p)]^{n-1} pe^t \\
E(X) = M'(0) = n[pe^0 + (1-p)]^{n-1} pe^0 \\
E(X) = M'(0) = n[p + (1-p)]^{n-1} p \\
E(X) = M'(0) = np[1]^{n-1} \\
E(X) = M'(0) = np \\
$$

|		The variance is the $V(X) = E(X^2) - E(X)^2$
$$

M''(t) =  n(n-1)[pe^t + (1-p)]^{n-2} p^2e^{2t} + n[pe^t + (1-p)]^{n-1} pe^t \\
E(X^2) = M''(0) = n(n-1)[pe^0 + (1-p)]^{n-2} p^2e^{0} + n[pe^0 + (1-p)]^{n-1} pe^0 \\
E(X^2) = M''(0) = n(n-1)p^2 + np \\
V(X) = E(X^2) - E(X)^2 = n(n-1)p^2 + np - (np)^2 \\
V(X) = np(1-p)

$$



# Question 3
Calculate the expected value and variance of the exponential distribution using the moment generating function.

|		The moment generation function for a exponential distribution can be described as follows
$$
f(x) = \lambda e^{\lambda x} \\
M_{x}(t) =  \frac{\lambda }{\lambda - t} , t < \lambda \\
M'(t) =  \frac{\lambda }{(\lambda - t)^2} \\
M''(t) =  \frac{2 \lambda }{(\lambda - t)^3} \\
$$




|		The expected value is the first moment with t set 0
$$
M'_{x}(t) = M'_{x}(0) = \frac{\lambda }{(\lambda - 0)^2}  = \frac{1}{\lambda} \\
$$



|		The variance is the $V(X) = E(X^2) - E(X)^2$
$$
M''(t) =  \frac{2 \lambda }{(\lambda - t)^3} \\
V(X) = E(X^2) - E(X)^2 = \frac{2 \lambda }{(\lambda - 0)^3} - \frac{1}{\lambda} \\
V(X) = \frac{2}{\lambda^2} - \frac{1}{\lambda} = \frac{1}{\lambda^2}  \\

$$



<br><br>
<b><i>Please show your work using an R-markdown document. Please name your assignment submission with your first initial and last name.</i></b>

